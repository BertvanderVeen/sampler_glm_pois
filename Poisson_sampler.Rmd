---
format: 
  html:
    page-layout: full
editor_options: 
  chunk_output_type: console
---

# Adopting a JAGS sampler
NIMBLE [de Valpine et al. 2017](https://www.tandfonline.com/doi/full/10.1080/10618600.2016.1172487) is a relatively new framework for fitting hierarchical models with MCMC. Previously, latent variable models were often fitted with JAGS [Plummer 2004](https://pdfs.semanticscholar.org/837b/9203abc8b3416e620d4c99d8500b4bd9be20.pdf) instead. An example for latent variable models, is the R-package Boral [Hui 2016](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12514). 

Boral allows users to straightforwardly, in familiar R interface, specify a latent variable model. This model specification is then translated to BUGS code (which JAGS uses) using some text manipulation. A while ago I decided that NIMBLE can probably sample the parameters for such a model more efficiently, as it allows for more flexibility in sampler specification and blocking schemes for parameters. Off I went; translating the script outputted by Boral in a format that can be read by NIMBLE was straightforward. Unfortunately, relative to NIMBLE's automated factor slice sampler, JAGS provided much more effective samples than NIMBLE.

One of the reasons for this, I suspect, if that JAGS has a larger library of MCMC samplers. Especially when it comes to models that can be represented as a GLM. This is also the case for latent variable models, because those are (almost) GLMs when keeping the latent variable fixed, and while estimating the other parameters. As a fairer comparison, I have set off to code up some GLM samplers from JAGS in NIMBLE. This is quite a challenge, since JAGS's samplers are coded in C++, and, well, my C++ is rusty at best. Fortunately, the [pogit R-package](https://cran.r-project.org/web/packages/pogit/index.html) includes chunks of the samplers in R-code, so I can use that when my understanding of C++ fails (and to verify that everything works correctly). It also does slab-and-spike variable selection, but I can ignore that bit for now.

## The Sampler

JAGS has not one GLM sampler. It might seem like that at first glance because JAGS's "GLM factory" has multiple scripts (such as [this one](https://github.com/todesking/JAGS-code/blob/master/src/modules/glm/samplers/GLMMethod.cc)) specific to GLMs. However, the idea is that when fed a model (script), JAGS first checks if that model can be represented as a linear model. If it can, it checks the distribution for the responses and is fed to a corresponding sampler for that distribution. Consequently, the "GLM sampler" is in fact a multitude of samplers. Each sampler is based on the same idea; the GLM can be represented as a linear model with some smart statistical voodoo.

For the Poisson case this is based on the work by [Fruhwirth-Schnatter and Wagner 2004](https://research.wu.ac.at/en/publications/data-augmentation-and-gibbs-sampling-for-regression-models-of-sma-7) and [Fruhwirth-Schnatter et al. 2009](https://link.springer.com/article/10.1007/s11222-008-9109-4).

The sampler relies on representing Poisson random variables in terms of their inter-arrival times, and approximating the distribution of the inter-arrival times by mixtures of normal distributions. Conditional on some quantities, the Poisson regression can be (approximately) represented as a linear regression. On each iteration of the sampler, the parameters are (conditionally) sampled from a multivariate normal distribution. Consequently, we can think of the sampler (algorithm) in three steps: 1) update parameters conditional on the inter-arrival times $\boldsymbol{\tau}_i$ and indicator variables $\boldsymbol{r}_1$ and $\boldsymbol{r}_2$. These latter quantities are treated as missing data.

The *pogit* R-package has functions to update these quantities; [**iams1_poisson**](https://github.com/cran/pogit/blob/b0f4b72eaabb1ce32efa1f4ce5517be7ece2afd0/R/dataug_pois_iams.R#L69) updates $\boldsymbol{\tau}$, [**iams2_poisson**](https://github.com/cran/pogit/blob/b0f4b72eaabb1ce32efa1f4ce5517be7ece2afd0/R/dataug_pois_iams.R#L97) updates $\boldsymbol{r}_1$ and $\boldsymbol{r}_2$ (and returns them in a concatenated vector). The posterior mean and covariance are calculated as a function of those quantities in [**select_poisson**](https://github.com/cran/pogit/blob/b0f4b72eaabb1ce32efa1f4ce5517be7ece2afd0/R/select_poisson.R#L110), where also a sampler from the posterior is drawn. The two additional functions [**mixcomp_poisson**](https://github.com/cran/pogit/blob/master/R/mixcomp_poisson.R) and [**compute_mixture**](https://github.com/cran/pogit/blob/b0f4b72eaabb1ce32efa1f4ce5517be7ece2afd0/R/compute_mixture.R) take care of the finite mixture approximation. **mixcomp_poisson** holds the necessary parameters for the finite mixture representation in the Poisson case, and **compute_mixture** uses those to calculate the approximation. The **mixcomp_poisson** function is (essentially) just a list, so I will ignore that for now and just call it from the package when I need it.

All these functions are present in JAGS as well, in: [AuxMixPoisson.CC](https://github.com/todesking/JAGS-code/blob/master/src/modules/glm/samplers/AuxMixPoisson.cc) and [AuxMixPoisson.h](https://github.com/todesking/JAGS-code/blob/master/src/modules/glm/samplers/AuxMixPoisson.h), and [LGMix.CC](https://github.com/todesking/JAGS-code/blob/master/src/modules/glm/samplers/LGMix.cc).

### Updating the inter-arrival times $\boldsymbol{\tau}$

Code from the **pogit** package includes R-functions that are challenging to incorporate in NIMBLE, so I built my functions from the ground up based on their code.

Here is a functiont to update $\boldsymbol{\tau}$:

```{r tau_func}
library(nimble)
update_tau = function(response.nodes = integer(1), n.response.nodes = integer(), n.zero.response.nodes = integer(), lambda = double(1)){
                        taunew = matrix(0, nrow = n.response.nodes, ncol = 2)
                        taunew[,1] <- rexp(n.response.nodes, lambda)
                        tau2 <- rbeta(n.response.nodes - n.zero.response.nodes, response.nodes[response.nodes>0], 1)
                        taunew[response.nodes > 0,  1] = 1-tau2 + taunew[response.nodes > 0,1]
                        taunew[response.nodes > 0,  2]  = tau2
                        taunew[response.nodes == 0, 1] = 1 +  taunew[response.nodes == 0, 1]
                        tau <<- taunew
                      }
```

I can simulate some data following a simple example:
```{r sim_dat}
N <- 1000 # number of observations
num.X = 10
set.seed(1);sigma <- rexp(1)
set.seed(1);beta1 <- rnorm(num.X+1, 0, sigma)
set.seed(1);X <- matrix(rnorm(n=N*num.X),ncol = num.X);X <- cbind(1,X)  # standard normal predictor
eta <- X%*%beta1
lambda <- exp(eta)  # link function
set.seed(1);y <- rpois(n=N, lambda=lambda)  # Poisson DV
```

and verify that it returns the same result as **pogit**:
```{r check_tau}
# calculate tau
set.seed(1);update_tau(response.nodes = y, n.response.nodes = N, n.zero.response.nodes = sum(y==0), lambda = lambda) # returns a matrix
# pogit needs these objects already here, they include properties of the data
mcomp <- pogit:::mixcomp_poisson()
compmix.pois <- pogit:::get_mixcomp(y = y, mcomp = mcomp)
set.seed(1);pogit_tau <- pogit:::iams1_poisson(y = y, mu = lambda, compmix.pois = compmix.pois) # returns a list of t1, t2

# calculate differences between vectors to verify correctness
sum(tau[,1] - pogit_tau$t1) # should be zero
sum(tau[y>0,2] - pogit_tau$t2) # should be zero
```

### Updating the indicator variables $\boldsymbol{r}_1$ and $\boldsymbol{r}_2$

So, tau is correct. Let us continue with a function for the indicator variables. The **pogit** package returns the two variables from the **iams2_poisson** function in a concatenated string, I have written two separate functions:

```{r R1_func}
update_R1 = function(n.response.nodes = integer(), lp = double(1), t1 = double(1), mcompm = double(2), mcompv = double(2), c1 = double(2))
                      {
                        minlogt1minlp <- matrix(-log(t1) - lp,ncol=10,nrow=n.response.nodes)
                        repmcompm  <- t(matrix(mcompm[1,], ncol = n.response.nodes, nrow = 10))
                        repmcompv <- t(matrix(mcompv[1,], ncol = n.response.nodes, nrow = 10))
                        rgm   <- c1 - 0.5*(minlogt1minlp - repmcompm)^2/repmcompv
                        for(i in 1:10){ #ncol rgm
                          rgm[rgm[,i]==0,i] <- -Inf
                        }
                        mx <- numeric(n.response.nodes)
                        e1 <- matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:n.response.nodes){
                          mx[i] <- max(rgm[i,])
                          if(mx[i]==-Inf)mx[i]=0
                          e1[i,] <- exp(rgm[i,]-mx[i])
                        }
                        rgmod <- numeric(n.response.nodes)
                        for(i in 1:n.response.nodes){
                          rgmod[i] <- sum(e1[i,])
                        }
                        e1.new = matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:10){
                          e1.new[,i] <- e1[,i]/rgmod # for safety might want to check for 0/0 i.e., nan
                        }
                        tri.mat = matrix(1, 10, 10)
                        for (j in 1:9) {
                          for (i in (j+1):10) {
                            tri.mat[i,j] = 0
                          }
                        }
                        Fn    <- e1.new%*%tri.mat
                        
                        # determination of random indicators R1
                        u <- runif(n.response.nodes, 0, 1)
                        R1.temp <- matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:10){
                          R1.temp[,i] <- u<Fn[,i] #can probably replace this step by "sample()"?
                        }
                        for(i in 1:n.response.nodes){
                          R1[i] <<- 11-sum(R1.temp[i,])
                        }
                      }
```
```{r R2_func}
update_R2 = function(response.nodes = integer(1), n.response.nodes = integer(), n.zero.response.nodes = integer(), lp = double(1), t2 = double(1), vy = double(2), my = double(2), wy = double(2)){
                        vy2 <- vy
                        for(i in 1:10){
                          vy2[vy2[,i]==0,i]<-1
                        }
                        lwy <- wy
                        for(i in 1:10){
                          lwy[lwy[,i]==0,i] <- 1
                          lwy[lwy[,i]<0,i] <- 1
                          lwy[,i] <- log(lwy[,i])
                        }
                        lvy <- vy
                        for(i in 1:10){
                          lvy[lvy[,i]==0,i] <- 1
                          lvy[lvy[,i]<0,i] <- 1
                          lvy[,i] <- log(lvy[,i])
                        }
                        c2 <- (lwy - 0.5*lvy) 
                        minlogt2minlp <- matrix(-log(t2) - lp[response.nodes>0],ncol=10,nrow=n.response.nodes-n.zero.response.nodes)
                        
                        kill <- matrix(vy > 0, n.response.nodes - n.zero.response.nodes, 10)
                        xx     <- minlogt2minlp*kill
                        rgmx   <- c2 - 0.5*(xx - my)^2/vy2
                        for(i in 1:10){ #ncol r gm
                          rgmx[rgmx[,i]==0,i] <- -Inf
                        }
                        e2 <- matrix(0,nrow = n.response.nodes-n.zero.response.nodes, ncol = 10)
                        mx2 = numeric(n.response.nodes-n.zero.response.nodes)
                        for(i in 1:(n.response.nodes-n.zero.response.nodes)){
                          mx2[i] = max(rgmx[i,])
                          if(mx2[i]==-Inf)mx2[i]=0
                          e2[i,] <- exp(rgmx[i,]-mx2[i])
                        }
                        rgmodx <- numeric(n.response.nodes-n.zero.response.nodes)
                        for(i in 1:(n.response.nodes-n.zero.response.nodes)){
                          rgmodx[i] <- sum(e2[i,])
                        }
                        e2.new = matrix(0,nrow=n.response.nodes-n.zero.response.nodes, ncol = 10)
                        for(i in 1:10){
                          e2.new[,i] <- e2[,i]/rgmodx # for safety might want to check for 0/0 i.e., nan
                        }
                        
                        tri.mat = matrix(1, 10, 10)
                        for (j in 1:9) {
                          for (i in (j+1):10) {
                            tri.mat[i,j] = 0
                          }
                        }
                        
                        Fx     <- e2.new%*%tri.mat
                        
                        # determination of random indicators R2
                        ux <- runif(n.response.nodes - n.zero.response.nodes, 0, 1)
                        R2.temp <- matrix(0, ncol = 10, nrow = n.response.nodes - n.zero.response.nodes)
                        for(i in 1:10){
                          R2.temp[,i] <- ux<Fx[,i]
                        }
                        
                        for(i in 1:(n.response.nodes-n.zero.response.nodes)){
                          R2[i] <<- 11-sum(R2.temp[i,])
                        }
                      }
```

Comparing again to the **pogit** package:

```{r check_R}
# first I compute a constant that both functions need
c1 <- t(matrix(log(mcomp$w[1, ]) - 0.5*log(mcomp$v[1, ]), nrow = 10, ncol = N))
# calculate my Rs
R1 <- integer(N)
R2 <- integer(sum(y>0))
set.seed(1);update_R1(n.response.nodes = N, lp = eta, t1 = pogit_tau$t1, mcompm = mcomp$m, mcompv = mcomp$v, c1 = c1)
set.seed(1);update_R2(response.nodes = y, n.response.nodes = N, n.zero.response.nodes = sum(y==0), lp = eta, t2 = pogit_tau$t2, vy = compmix.pois$vy, my = compmix.pois$my, wy = compmix.pois$wy)
# pogit Rs
cm1 <- list(comp = list(m = mcomp$m[1, ], v = mcomp$v[1, ], w = mcomp$w[1, ]), c1 = c1) # pogit requires a list of arguments
set.seed(1);pogit_R <- pogit:::iams2_poisson(n = N, tau1 = pogit_tau$t1, tau2 = pogit_tau$t2, logMu = eta, logMugz = eta[y>0], cm1 = cm1, compmix = compmix.pois)

sum(pogit_R[1:N]-R1) # should be zero
sum(pogit_R[-c(1:N)]-R2) # should be zero 
```

the output for both sums should be zero, but for $\boldsymbol{r}_2$ is not. I suspect this is because the seed for *pogit_R* is not set correctly, since the function includes two random number generators and mine includes only one. When using a number generator twice in sequence inside a function, *R* "acts" like the second sequence is a continuation of the first. In contrast, because I split the two up into two separate functions, *R* acts like they are both unique sequences. Anyway, I have verified that the calculation for $\boldsymbol{r}_2$ is correct.

### Posterior moments

All that remains is to calculate the posterior moments and generate a sample from the posterior. Unfortunately, this is harder to reproduce with the **pogit** package because there is no function that returns those quantities. In an attempt to verify my output anyway, I have written a function that outputs the posterior mean, covariance, and a proposal, based on the code in **select_poisson.R**:

```{r pogit_post_calc}
pogit_post_calc <- function(X, compmix, tau1, tau2, R, cm1){
X <- as.matrix(X)
n <- nrow(X)
# mixture component means and variances
m1 <- cm1$comp$m[R[1:n]]
m2 <- compmix.pois$my[cbind(seq_len(compmix.pois$ngz), R[-(1:n)])]
mR <- as.matrix(c(m1,m2), (n + compmix.pois$ngz))
v1 <- cm1$comp$v[R[1:n]]
v2 <- compmix.pois$vy[cbind(seq_len(compmix.pois$ngz), R[-(1:n)])]
invSig <- 1/sqrt(c(v1,v2))

# stacking and standardizing
tauS <- c(tau1, tau2)
xS <- rbind(X, X[compmix.pois$igz, , drop = FALSE])
yS <- (-log(tauS) - mR)*invSig

Xall <- xS*kronecker(matrix(1, 1, ncol(X)), invSig) # X times inv. sd.dev of mixtures

# inv. prior variance
invA0 <- diag(ncol(Xall))
a0 = rep(0,ncol(Xall)) # prior mean

AP    <- solve(invA0 + t(Xall)%*%Xall)  # A = (A0^-1 + (Z*)'Sigma^-1 Z*)
aP    <- AP%*%(invA0%*%a0 + t(Xall)%*%yS) # a = A(A0^-1*a0 + (Z*)'Sigma^-1*y
zetaP <- t(chol(AP))%*%matrix(rnorm(ncol(Xall)), ncol(Xall), 1) + aP 
return(list(postMean = aP, postCov = AP, proposal = zetaP))
}
```

and my function, only for testing purposes here:

```{r my_post_calc}
calc_post_mom <- function(X = double(2), n.response.nodes = integer(), n.zero.response.nodes = integer(), n.param.nodes = integer(), response.nodes = integer(1), R1 = double(1), R2 = double(1), tau = double(2), mcompv = double(2), mcompm = double(2), my = double(2), vy = double(2), priorMean = double(1), priorCov = double(2), lpCon = double(1)){
    m1 <- mcompm[1,R1]
    m2 <- numeric(n.response.nodes-n.zero.response.nodes)
    for(i in 1:(n.response.nodes-n.zero.response.nodes)){
      m2[i] <- my[i,R2[i]]
    }
    # second: mixture component variances
    v1 <- mcompv[1,R1]
    v2 <- numeric(n.response.nodes-n.zero.response.nodes)
    for(i in 1:(n.response.nodes-n.zero.response.nodes)){
      v2[i] <- vy[i,R2[i]]
    }
    invSigS = numeric(n.response.nodes)
    ys1 = (-log(tau[,1])-m1 - lpCon)/sqrt(v1) # this is also "AuxMixPoisson:value()" in JAGS
    ys2 = (-log(tau[response.nodes>0,2])-m2 - lpCon[response.nodes>0])/sqrt(v2)
    invSigS = 1/v1
    invSigS[response.nodes>0] = invSigS[response.nodes>0]+1/v2
    # posterior parameters
    postCov <<- inverse(inverse(priorCov) + t(X)%*%diag(invSigS)%*%X)
    postMean <<- postCov%*%(solve(priorCov, priorMean)+ (t(X)%*%diag(1/sqrt(v1))%*%ys1+t(X[response.nodes>0,])%*%diag(1/sqrt(v2))%*%ys2)) # see eq (8) Fruhwirth-Schnatter and Wagner 2006 or L109-111 from pogit::select_poisson.Rs
}
```

test it:
```{r test_post_calc}
set.seed(1);my_post <- calc_post_mom(X, N, sum(y==0), 2, y, pogit_R[1:N], pogit_R[-c(1:N)], tau, mcomp$v, mcomp$m, compmix.pois$my, compmix.pois$vy, rep(0,ncol(X)), diag(ncol(X)), rep(0,N))
set.seed(1);pogit_post <- pogit_post_calc(X,compmix,pogit_tau$t1, pogit_tau$t2, pogit_R, cm1)
sum(postMean-pogit_post$postMean)
sum(postCov-pogit_post$postCov)
```

### Testing

Now we can try running the functions together, and get some posterior samples:

```{r run_mcmc}
# define some constants
    # number of target parameters
    response.nodes <- y
    n.param.nodes <- dim(X)[2]
    n.response.nodes <- dim(X)[1]
    n.zero.response.nodes <- sum(y==0)
    # mixture components
    # v: variances (s), m: means (m), w: weights
    mcomp <- pogit:::mixcomp_poisson()
    mcompm <- mcomp$m
    mcompv <- mcomp$v
    mcompw <- mcomp$w
    # initialize R, indicator variables
    R1 <- rep(NA_real_, n.response.nodes)
    R2 <- rep(NA_real_, n.response.nodes-n.zero.response.nodes)
    tau <- matrix(NA_real_, n.response.nodes,ncol=2)
    # start by getting mixture components, this is "get_mixcomp" in pogit
    # m: means for mixture components, v: scales, w: weights
    if(sum(response.nodes<3e4)==n.response.nodes){
      my = mcomp$m[response.nodes[response.nodes>0],]
      vy = mcomp$v[response.nodes[response.nodes>0],]
      wy = mcomp$w[response.nodes[response.nodes>0],]
    }else{
      wy = vy = my = matrix(0, nrow = n.response.nodes - n.zero.response.nodes, ncol = 10)
      my[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$m)[2]] = mcomp$m[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      vy[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$v)[2]] = mcomp$v[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      wy[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$w)[2]] = mcomp$w[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      # this comes from "compute_mixture.R"
      for(i in response.nodes[response.nodes>3e4]){
        my[response.nodes[response.nodes>0]==i,1] <- -digamma(i)
        vy[response.nodes[response.nodes>0]==i,1] <- trigamma(i)
        wy[response.nodes[response.nodes>0]==i,1] <- 1 
      }
    }
    vy[is.na(vy)] <- 0
    my[is.na(my)] <- 0
    wy[is.na(wy)] <- 0
    # for use in step 3
    c1 <- t(matrix(log(mcompw[1, ]) - 0.5*log(mcompv[1, ]), nrow = 10, ncol = n.response.nodes)) # not sure what this is yet
n.iter = 1e3
samples <- NULL
beta <- rnorm(ncol(X)) # initialize
lp = X%*%beta # create linear predictor
lambda = exp(lp) # distributional parameter
update_tau(response.nodes, N, n.zero.response.nodes, lambda) # intitialize tau
update_R1(N, lp, tau[,1], mcomp$m, mcompv = mcomp$v, c1)# initialize R1
update_R2(response.nodes, N, n.zero.response.nodes, lp, tau[y>0,2], vy, my, wy)  # initialize R2
pb = txtProgressBar(min = 0, max = n.iter, initial = 0) # progress bar. because, why not!

for(i in 1:n.iter){
# calculate posterior moments
  calc_post_mom(X, N, n.zero.response.nodes, n.param.nodes, y, R1, R2, tau, mcomp$v, mcomp$m, my, vy, rep(0,ncol(X)), diag(ncol(X)), rep(0,n.response.nodes))
  # simulate from posterior
  beta <- rmnorm_chol(1, mean = c(postMean), cholesky = chol(postCov), prec_param = 0)
  # store sample
  samples <- rbind(samples, beta)
  # update lp, lambda
  lp = X%*%beta # linear predictor
  lambda = exp(lp) # distributional parameter
  update_tau(response.nodes, N, n.zero.response.nodes, lambda) # update tau
  update_R1(N, lp, tau[,1], mcomp$m, mcompv = mcomp$v, c1)# update R1
  update_R2(response.nodes, N, n.zero.response.nodes, lp, tau[y>0,2], vy, my, wy)  # update R2
  setTxtProgressBar(pb,i)
}
summary(coda::as.mcmc(samples)) # correct!
```

### Putting it in NIMBLE

Nimble runs things in C++ so it will be faster, and we can combine it with other samplers and fun tricks. So, is the code in Nimble, which includes some changes to make things play nice with samplers on other parameters:

```{r nim_samp1}
sampler_glm_pois <- nimbleFunction(
  
  contains = sampler_BASE,
  name = 'sampler_glm_pois',
  setup = function(model, mvSaved, target, control) {
    # some defensive programming
    # assigning the sampler to only some components of a multivariate node will
    # result in the wrong posterior
    targetCollected <- model$expandNodeNames(target) # not sure if multivariate nodes can ever come as series of univariates
    n.target.collected <- length(targetCollected)
    targetDnormCollected <- model$expandNodeNames(target[model$getDistribution(target)=="dnorm"])
    n.target.dnorm.collected <- length(targetDnormCollected)
    targetDmnormCollected <- model$expandNodeNames(target[model$getDistribution(target)=="dmnorm"])
    n.target.dmnorm.collected <- length(targetDmnormCollected)
    idx.dnorm <- which(model$getDistribution(targetCollected)=="dnorm")
    idx.dmnorm <- which(model$getDistribution(targetCollected)=="dmnorm")

    targetAsScalar <- model$expandNodeNames(target, returnScalarComponents = TRUE)
    n.param.nodes <- length(targetAsScalar) # number of target parameters
    if(n.target.collected != n.param.nodes){
      stop("This sampler should be applied to all components of a multivariate node simultaneously.")
    }
    priorDist <- unlist(sapply(targetCollected,model$getDistribution))
    # find hyper parameter index
    priorParID <- integer(n.target.collected)
    for(i in 1:n.target.collected){
      priorParID[i] <- which(names(getDistributionInfo(model$getDistribution(targetCollected[i]))$paramIDs)%in%c("var","cov"))
    }
    # this sampler only works for normally distributed nodes
    if(!all(priorDist%in%c("dnorm","dmnorm"))){
      stop("This sampler requires target parameters to have normally distributed priors.")
    }
    calcNodes <- model$getDependencies(target)
    targetDeps <- model$getDependencies(targetAsScalar,includeData = F,self=F)
    # check if initial values were assigned, if not simulate from prior
    if(is.na(model$calculate())){
    model$simulate()
    model$calculate()
    }
    # get some information about the model
    response.nodes.names <- names(which(model$isStoch(names(model$origData)))) # names of response nodes
    if(!all(model$getDistribution(response.nodes.names)=="dpois"))stop("This sampler is designed only for Poisson responses.")
    response.name  = unique(gsub("\\[.*","",response.nodes.names))
    response.nodes <- values(model, response.nodes.names) # response data
    n.response.nodes <- length(response.nodes) # number of observations
    n.zero.response.nodes = sum(response.nodes==0) # number of zero observations
    ## get distribution parameter names
    dist.par.names = model$getParents(response.nodes.names,determOnly = F, immediateOnly = T)
    distpar.name  = unique(gsub("\\[.*","",dist.par.names))
    # lp.nodes.names = model$getParents(dist.par.names,immediateOnly = T, determOnly = T,includeData = F)
    # lp.name  = unique(gsub("\\[.*","",lp.nodes.names))
    # get corresponding "data" nodes to target
    # we don't have information on the data, so we do this by iterating through the linear predictor
    # parameter by parameter
    inits.setup <- values(model, targetAsScalar) # temporarily store parameters
    # get any constant parts in the lp first
    lpCon <- double(length=n.response.nodes)
    values(model, targetAsScalar) <- rep(0,n.param.nodes)
    model$calculate(targetDeps)
    lpCon <- log(values(model, dist.par.names))
    values(model, targetAsScalar) <- inits.setup # restore parameters
    model$calculate(targetDeps)
    # calculate "X" from the linear predictor
    X <- matrix(0, nrow = n.response.nodes, ncol = n.param.nodes)
    xBool <- matrix(FALSE, nrow = n.response.nodes, ncol = n.param.nodes)
    lp1.temp.setup <- log(values(model, dist.par.names))
    dep.list <- list()
    # calcX <- nimbleFunctionList(calcNodes)
    for(i in 1:n.param.nodes){
     # we start by finding nodes of the LP that depend on targetAsScalar[i]
     # the rest we do not need to update; they are zero
     # then we only have re-calculate a limited set of nodes on each iteration
    xBool[,i] <- dist.par.names%in%model$getDependencies(targetAsScalar[i])
    # generate function to recalculate these nodes plus dependencies
    dep.list[[i]] <- model$getParents(dist.par.names[xBool[,i]], includeData = F, self = T)
    # calcX[[i]] <- calcNodes(model, model$getDependencies(dist.par.names[dist.par.bool.Xi],includeData = F))
    par.setup <- inits.setup[i]
    values(model, targetAsScalar[i]) <- 0
    model$calculate(model$getDependencies(targetAsScalar[i],includeData = F))
    lp2.temp.setup <- log(values(model, dist.par.names))
    values(model, targetAsScalar[i]) <- inits.setup[i]
    model$calculate(model$getDependencies(targetAsScalar[i],includeData = F))
    X[,i] <- (lp1.temp.setup-lp2.temp.setup)/par.setup # only works if model is linear on the link scale (like a glm)
    }
    
    if(any(apply(X,2,function(x)any(is.nan(x))))||any(apply(X,2,function(x)any(is.na(x)))))stop("Failed to find covariates from model. Might some columns be full of zeros?")
    
    rm(lp2.temp.setup,lp1.temp.setup, par.setup)
    # Set-up the prior parameters
    priorMean = matrix(0,ncol=1,nrow=n.param.nodes)
    priorCov = matrix(0,ncol = n.param.nodes, nrow = n.param.nodes)
    # need to turn this into a function that accepts a bunch of indices so i can update hyperpars mid-run
    # if all node entries are in the target, sample as is. otherwise, we need the conditional means and covariance.
    # so, check if "target" has subset entries of a multivariate node
    # means
      k.setup <- 1
      for(i in 1:n.target.collected){
      means.setup = model$getParam(targetCollected[i], 'mean')
      priorMean[k.setup:(k.setup+length(means.setup)-1),1] <- means.setup
      k.setup <- k.setup+length(means.setup)
      }
      rm(means.setup, k.setup)
    # covariances
      k.setup <- 1
      for(i in 1:n.target.collected){
      covs.setup = as.matrix(model$getParam(targetCollected[i], priorParID[i]))
      dims.setup = dim(covs.setup)[1]
      priorCov[k.setup:(k.setup+dims.setup-1),k.setup:(k.setup+dims.setup-1)] <- covs.setup
      k.setup <- k.setup+dims.setup
      }
      rm(covs.setup, k.setup, dims.setup)
    # check if there are hyperparameters in the model that might need updating mid-run
    hasHyperPar <- length(model$getParents(target))>0
    # check if there are other parameters in the model than that we are sampling
    # but filter it by potential parent nodes, i.e., hyper parameters
    hasOtherLPpar <- (length(model$expandNodeNames(model$getNodeNames(stochOnly = T,includeData = F)))-length(model$getParents(model$expandNodeNames(model$getNodeNames(stochOnly = T,includeData = F)))))>length(targetAsScalar)
    # mixture components
    # v: variances (s), m: means (m), w: weights
    mcomp <- pogit:::mixcomp_poisson()
    mcompm <- mcomp$m
    mcompv <- mcomp$v
    mcompw <- mcomp$w
    # start by getting mixture components, this is "get_mixcomp" in pogit
    # m: means for mixture components, v: scales, w: weights
    if(sum(response.nodes<3e4)==n.response.nodes){
      my = mcomp$m[response.nodes[response.nodes>0],]
      vy = mcomp$v[response.nodes[response.nodes>0],]
      wy = mcomp$w[response.nodes[response.nodes>0],]
    }else{
      wy = vy = my = matrix(0, nrow = n.response.nodes - n.zero.response.nodes, ncol = 10)
      my[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$m)[2]] = mcomp$m[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      vy[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$v)[2]] = mcomp$v[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      wy[response.nodes[response.nodes>0] <= 3e4,1:dim(mcomp$w)[2]] = mcomp$w[response.nodes[(response.nodes>0) & (response.nodes <= 3e4)],]
      # this comes from "compute_mixture.R"
      for(i in response.nodes[response.nodes>3e4]){
        my[response.nodes[response.nodes>0]==i,1] <- -digamma(i)
        vy[response.nodes[response.nodes>0]==i,1] <- trigamma(i)
        wy[response.nodes[response.nodes>0]==i,1] <- 1 
      }
    }
    vy[is.na(vy)] <- 0
    my[is.na(my)] <- 0
    wy[is.na(wy)] <- 0
    # for use in step 3
    # R1
    c1 <- t(matrix(log(mcompw[1, ]) - 0.5*log(mcompv[1, ]), nrow = 10, ncol = n.response.nodes)) # not sure what this is yet
    # R2
    vy2 <- vy
    vy2[vy2==0]<-1
    lwy <- log(wy)
    lwy[is.infinite(lwy)] <- 0
    lvy <- log(vy)
    lvy[is.infinite(lvy)] <- 0
    c2 <- (lwy - 0.5*lvy) 
    kill <- matrix(vy > 0, n.response.nodes - n.zero.response.nodes, 10)
  },
  run = function() {
    # update "covariates"
    inits <- values(model, targetAsScalar)
    for(i in 1:n.param.nodes){
    par <- inits[i]
    values(model, targetAsScalar[i]) <<- 0
    model$calculate(dep.list[[i]])
    temp.lp1 <- log(values(model, dist.par.names))
    values(model, targetAsScalar[i]) <<- inits[i]
    model$calculate(dep.list[[i]])
    X[,i] <<- (lp.temp.setup-temp.lp1)/par # only works if model is linear on the link scale (like a glm)
    }
    
    # -1. Update "lpCon", i.e., "fixed" portion of LP
    if(hasOtherLPpar){
    pars <- values(model, target) # temporarily store parameters
    lp.temp <- log(values(model, dist.par.names))
    values(model, targetAsScalar) <<- rep(0, n.param.nodes)
    model$calculate(targetDeps)
    lpCon <<- log(values(model, dist.par.names))
    values(model, targetAsScalar) <<- pars # restore parameters
    model$calculate(targetDeps)
    }
    
    # 0. retrieve new hyperparameters
    if(hasHyperPar){
      k <- 1L
      for(i in 1:n.target.collected){
      dims <- length(values(model, targetCollected[i]))
      idx <- k+dims-1L
      priorMean[k:idx,1] <<- nimMatrix(model$getParam(targetCollected[i], 'mean'),ncol = 1)
      k <- k+dims
      }
      if(n.target.dnorm.collected >0){
      # because nimble will by default treat a length 1 vector as a scalar
        vec.idx.dnorm <- integer(value = idx.dnorm, length = n.target.dnorm.collected)
      for(i in 1:n.target.dnorm.collected){
      priorCov[vec.idx.dnorm[i],vec.idx.dnorm[i]] <<-  model$getParam(targetDnormCollected[i], 'var')
      }        
      }
      if(n.target.dmnorm.collected > 0){
        # because nimble will by default treat a length 1 vector as a scalar
        vec.idx.dmnorm <- integer(value = idx.dmnorm, length = n.target.dmnorm.collected)
      for(i in 1:n.target.dmnorm.collected){
      dims = length(values(model, targetDmnormCollected[i]))
      idx <- vec.idx.dmnorm[i]+dims-1L
      priorCov[vec.idx.dmnorm[i]:idx,vec.idx.dmnorm[i]:idx] <<-  nimMatrix(model$getParam(targetDmnormCollected[i], 'cov'), ncol = dims, nrow = dims)
      }        
      }
    }

    ## get LP and lambda
    lambda = values(model, dist.par.names)
    lp = log(lambda)# sampler is only valid for models with the log-link # values(model, dist.par.names)
    
    #1. update tau's, step 1 in Fruhwirth-Schnatter et al. 2009
    # this part is covered in dataug_pois_iams.R->iams1_poisson
    # tau are latent variables that represent the inter-arrival times of a Poisson process
    # get_mixcomp_poisson in dataug_pois_iams.R just gets components that we need for the other functions
    tau <- update_tau(response.nodes, n.response.nodes, n.zero.response.nodes, lambda)
    t1 <- tau[,1]
    t2 <- tau[response.nodes>0,2]
    
    # 2. update R's, step 2 in Fruhwirth-Schnatter et al. 2009
    # this part is covered in dataug_pois_iams.R->iams2_poisson
    # Rmix are normal mixture approximations to Poissons
    R1 <- update_R1(n.response.nodes, lp, t1, mcompm, mcompv, c1)
    R2 <- update_R2(response.nodes, n.response.nodes, n.zero.response.nodes, lp, t2, vy, my, vy2, c2, kill)
    
    # 3. simulate parameters given R, tau
    # this part is covered in "select_poisson.R"
    # calculate posterior moments
    proposal <- simulate_post(X, n.response.nodes, n.zero.response.nodes, n.param.nodes, response.nodes, R1, R2, tau, mcompv, mcompm, my, vy, priorMean, priorCov, lpCon)
    # store it
    values(model, targetAsScalar) <<-  proposal
    # update nodes based on proposal
    model$calculate()
    # keep the model and mvSaved objects consistent
    copy(from = model, to = mvSaved, row = 1, 
         nodes = target, logProb = F)
  },
  methods = list(
    simulate_post = function(X = double(2), n.response.nodes = integer(), n.zero.response.nodes = integer(), n.param.nodes = integer(), response.nodes = integer(1), R1 = integer(1), R2 = integer(1), tau = double(2), mcompv = double(2), mcompm = double(2), my = double(2), vy = double(2), priorMean = double(2), priorCov = double(2), lpCon = double(1)){
    # first: mixture component means  
    m1 <- mcompm[1,R1]
    m2 <- numeric(n.response.nodes-n.zero.response.nodes)
    for(i in 1:10){
      m2[R2==i] <- my[R2==i, i]
    }
    # second: mixture component variances
    v1 <- mcompv[1,R1]
    v2 <- numeric(n.response.nodes-n.zero.response.nodes)
    for(i in 1:10){
      v2[R2==i] <- vy[R2==i, i]
    }
    invSigS = numeric(n.response.nodes)
    ys1 = (-log(tau[,1])-m1 -lpCon)/sqrt(v1) # this is also "AuxMixPoisson:value()" in JAGS
    ys2 = (-log(tau[response.nodes>0,2])-m2-lpCon[response.nodes>0])/sqrt(v2)
    invSigS = 1/v1
    invSigS[response.nodes>0] = invSigS[response.nodes>0]+1/v2
    # prior parameters
    priorPrec <- inverse(priorCov)
    # posterior parameters
    postPrec <- priorPrec + t(X)%*%diag(invSigS)%*%X
    postMean <- solve(postPrec, priorPrec%*%priorMean+ (t(X)%*%diag(1/sqrt(v1))%*%ys1+t(X[response.nodes>0,])%*%diag(1/sqrt(v2))%*%ys2)) # see eq (8) Fruhwirth-Schnatter and Wagner 2006 or L109-111 from pogit::select_poisson.Rs
    # a sample from the posterior
    proposal <- rmnorm_chol(1, mean = c(postMean), cholesky = chol(postPrec), prec_param = TRUE)
    return(proposal)
    returnType(double(1))
},
                 update_tau = function(response.nodes = integer(1), n.response.nodes = integer(), n.zero.response.nodes = integer(), lambda = double(1)){
                        taunew = matrix(0, nrow = n.response.nodes, ncol = 2)
                        taunew[,1] <- rexp(n.response.nodes, lambda)
                        tau2 <- rbeta(n.response.nodes - n.zero.response.nodes, response.nodes[response.nodes>0], 1)
                        taunew[response.nodes > 0,  1] = 1-tau2 + taunew[response.nodes > 0,1]
                        taunew[response.nodes > 0,  2]  = tau2
                        taunew[response.nodes == 0, 1] = 1 +  taunew[response.nodes == 0, 1]
                        return(taunew)
                        returnType(double(2))
                      },
                update_R1 = function(n.response.nodes = integer(), lp = double(1), t1 = double(1), mcompm = double(2), mcompv = double(2), c1 = double(2))
                      {
                        rgm <- matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:10){
                          rgm[,i] <- c1[,i] - 0.5*(-log(t1)-lp - mcompm[1,i])^2/mcompv[1,i]
                          rgm[rgm[,i]==0,i] <- -Inf
                        }
                        mx <- numeric(n.response.nodes)
                        e1 <- matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:n.response.nodes){
                          mx[i] <- max(rgm[i,])
                          if(mx[i]==-Inf)mx[i]=0
                          e1[i,] <- exp(rgm[i,]-mx[i])
                        }
                        # row sum
                        rgmod <- e1%*%matrix(1,nrow=10,ncol=1)
                        
                        e1.new = matrix(0, ncol = 10, nrow = n.response.nodes)
                        for(i in 1:10){
                          e1.new[,i] <- e1[,i]/rgmod # for safety might want to check for 0/0 i.e., nan
                        }
                        tri.mat = matrix(1, 10, 10)
                        for (j in 1:9) {
                          for (i in (j+1):10) {
                            tri.mat[i,j] = 0
                          }
                        }
                        Fn    <- e1.new%*%tri.mat
                        
                        # determination of random indicators R1
                        # inverse transform sampling Poisson-Binomial
                        u <- runif(n.response.nodes, 0, 1)
                        R1.temp <- nimMatrix(0, ncol = 10, nrow = n.response.nodes, type = "integer")
                        for(i in 1:10){
                          R1.temp[,i] <- u<Fn[,i]
                        }
                        # declare R1new explicitly as integer
                        R1new <- integer(n.response.nodes)
                        R1new[1:n.response.nodes] <- 11L-c(R1.temp%*%nimMatrix(1,nrow=10,ncol=1, type = "integer"))
                        
                        return(R1new)
                        returnType(integer(1))
                      },
                update_R2 = function(response.nodes = integer(1), n.response.nodes = integer(), n.zero.response.nodes = integer(), lp = double(1), t2 = double(1), vy = double(2), my = double(2), vy2 = double(2), c2 = double(2), kill = logical(2)){
                        rgmx <- matrix(0, ncol = 10, nrow = n.response.nodes - n.zero.response.nodes)
                        # log(dnorm(-t2-lp, my, vy2)) but with correct normalisation, c2
                        for(i in 1:10){ #ncol r gm
                          rgmx[,i]   <- c2[,i] - 0.5*((-log(t2)-lp[response.nodes>0]- my[,i])*kill[,i])^2/vy2[,i]
                          rgmx[rgmx[,i]==0,i] <- -Inf
                        }
                        e2 <- matrix(0,nrow = n.response.nodes-n.zero.response.nodes, ncol = 10)
                        mx2 = numeric(n.response.nodes-n.zero.response.nodes)
                        for(i in 1:(n.response.nodes-n.zero.response.nodes)){
                          mx2[i] = max(rgmx[i,])
                          if(mx2[i]==-Inf)mx2[i]=0
                          e2[i,] <- exp(rgmx[i,]-mx2[i])
                        }
                        # row sum; cumulative probabilities
                        rgmodx <- e2%*%matrix(1,nrow=10,ncol=1)

                        e2.new = matrix(0,nrow=n.response.nodes-n.zero.response.nodes, ncol = 10)
                        for(i in 1:10){
                          e2.new[,i] <- e2[,i]/rgmodx # for safety might want to check for 0/0 i.e., nan
                        }
                        
                        tri.mat = matrix(1, 10, 10)
                        for (j in 1:9) {
                          for (i in (j+1):10) {
                            tri.mat[i,j] = 0
                          }
                        }
                        
                        Fx     <- e2.new%*%tri.mat
                        
                        # determination of random indicators R2
                        # inverse transform sampling Poisson-Binomial
                        ux <- runif(n.response.nodes - n.zero.response.nodes, 0, 1)
                        R2.temp <- nimMatrix(0, ncol = 10, nrow = n.response.nodes - n.zero.response.nodes, type = "integer")
                        for(i in 1:10){
                          R2.temp[,i] <- ux<Fx[,i]
                        }
                        # declare R2new explicitly as integer
                        R2new <- integer(n.response.nodes - n.zero.response.nodes)
                        R2new[1:(n.response.nodes-n.zero.response.nodes)] <- 11L-R2.temp%*%nimMatrix(1,nrow=10,ncol=1, type = "integer")
                        
                        return(R2new)
                        returnType(integer(1))
                      },
                 reset = function () {}
                    )
)
```

```{r nim_samp, echo =F, include = F, eval = FALSE}
# get all parameters
    allPar <- model$expandNodeNames(model$getNodeNames(stochOnly = T,includeData = F))
    # exclude hyper parameters
    allPar <- allPar[!allPar%in%model$getParents(model$expandNodeNames(model$getNodeNames(stochOnly = T,includeData = F)))]
```

write some BUGS code for a simple Poisson regression;

```{r nim_script}
library(nimble)
pois.glm <- nimbleCode({
  ## Likelihood
  for(i in 1:N){
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- eta[i]
    eta[i] <- inprod(beta[1:k],X[i,])
  }
   for(l in 1:k){
    beta[l] ~ dnorm(0,sd = sigma)
  }
  sigma ~ dexp(1)
})
```

set-up the model:

```{r setup_model}
dat <- list(X=X,  # predictors
                y=y)  # DV
const <- list(
                N=N,  # sample size
                k =ncol(dat$X)) # number of covariates,
# construct model object
model <- nimbleModel(pois.glm, const, dat, buildDerivs = T, inits = list(beta = rnorm(num.X+1), sigma  =0.1))
mod <- nimble::compileNimble(model)
```


<!-- test if everything works: -->
<!-- ```{r, test_it} -->
<!-- test_sampler<-nimble::compileNimble(sampler_glm_pois(model,model,"beta",list())) -->
<!-- update_tau(y,test$n.response.nodes, test$n.zero.response.nodes, lambda = values(model, "lambda")) -->
<!-- test$tau<-test$update_tau(test$response.nodes,test$n.response.nodes, test$n.zero.response.nodes, lambda = values(model, "lambda")) -->
<!-- update_R1(test$n.response.nodes,lp=values(model,"eta"), test$tau[,1], test$mcompm, test$mcompv, test$c1) -->
<!-- test$update_R1(test$n.response.nodes,lp=values(model,"eta"), test$tau[,1], test$mcompm, test$mcompv, test$c1) -->
<!-- test$update_R2(test$response.nodes, test$n.response.nodes, test$n.zero.response.nodes, values(model, "eta"), test$tau[test$response.nodes>0,2], test$c2, test$kill) -->
<!-- ``` -->

and run it for a few iterations to see that we get something sensible:

```{r run_my_mcmc}
library(nimbleHMC)
nimbleMCMCconf <- configureMCMC(mod, monitors = c("beta","sigma"), print = FALSE)
nimbleMCMCconf$removeSamplers()
#sampler does not yet play nice together  with RW..
# sapply(1:5,function(i)nimbleMCMCconf$addSampler(paste0("beta[",i,"]"),type="sampler_RW", print = T))
# nimbleMCMCconf$addSampler("beta[1:5]",type="sampler_glm_pois", print = T)
# nimbleMCMCconf$addSampler("beta[6:11]",type="sampler_glm_pois", print = T)
sapply(1:11,function(i) nimbleMCMCconf$addSampler(paste0("beta[",i,"]"),type="sampler_glm_pois", print = T))
nimbleMCMCconf$addSampler("sigma",type="sampler_NUTS", print = T)
nimbleMCMCb <- buildMCMC(nimbleMCMCconf)
nimbleMCMCc <- compileNimble(nimbleMCMCb, project = mod)
test_run <- runMCMC(nimbleMCMCc, niter = 1000, nburnin = 500, thin = 1, nchains = 1, samplesAsCodaMCMC = TRUE)

cbind(est=summary(test_run)[1]$statistics[,1], true = c(beta1,sigma))
```

<!-- compare the solution to *pogit*: -->
<!-- ```{r run_pogit_mcmc} -->
<!-- pogit_run <- pogit::poissonBvs(y, X = cbind(1,x), BVS = FALSE, mcmc = list(M = 1000, burnin = 500, thin = 1)) -->
<!-- summary(pogit_run$samplesP$beta) # correct answer -->
<!-- ``` -->

## GLLVM

First we try a multivariate example:
```{r ex2}
N <- 1000 # number of observations
p <- 5 # number of columns in response
num.X = 10
set.seed(1);sigma <- rexp(p)
set.seed(1);beta1 <- matrix(rnorm((num.X+1)*p, 0, sigma),ncol=p)
set.seed(1);X <- matrix(rnorm(n=N*num.X),ncol = num.X);X <- cbind(1,X)  # standard normal predictor
eta <- X%*%beta1
lambda <- exp(eta)  # link function
set.seed(1);y <- matrix(rpois(n=N*p, lambda=lambda),ncol=p)  # Poisson DV

pois.glm2 <- nimbleCode({
  ## Likelihood
  for(j in 1:p){
  for(i in 1:N){
    y[i,j] ~ dpois(lambda[i,j])
    log(lambda[i,j]) <- eta[i,j]
    eta[i,j] <- inprod(beta[,j],X[i,])
  }
   for(l in 1:k){
    beta[l,j] ~ dnorm(0,sd = sigma[j])
  }
  sigma[j] ~ dexp(1)
  }
})

dat <- list(y = y)
const <- list(
 p = ncol(dat$y),
 N = nrow(dat$y), 
 X = X, 
 k = ncol(X))
# construct model object
model <- nimbleModel(pois.glm2, const, dat, dimensions = list(beta = c(const$k,const$p)), buildDerivs = T)
mod <- nimble::compileNimble(model)
# assign sampler
library(nimbleHMC)
nimbleMCMCconf <- configureMCMC(mod, monitors = c("beta","sigma"), print = FALSE)
nimbleMCMCconf$removeSamplers()
nimbleMCMCconf$addSampler("sigma",type="sampler_NUTS", print = T)
sapply(1:11,function(j)nimbleMCMCconf$addSampler(paste0("beta[",j,", 1:5]"),type="sampler_glm_pois", print = T))
nimbleMCMCb <- buildMCMC(nimbleMCMCconf)
nimbleMCMCc <- compileNimble(nimbleMCMCb, project = mod)
test_run <- runMCMC(nimbleMCMCc, niter = 1000, nburnin = 500, thin = 1, nchains = 1, samplesAsCodaMCMC = TRUE)
```

Try using the sampler to estimate a simple GLLVM. Code gener from Boral, so that we can compare to the results from JAGS.

```{r gllvm_jags, include = F, echo= F, eval = F}
data(spider, package = "mvabund")
pois.gllvm <- nimbleCode ({
  
  ## Nimble model written for boral version 2.0 on 2023-09-21 14:23:11.020833
  ## Data Level ## 
  for(i in 1:n) {
    for(j in 1:p) { eta[i,j] <- inprod(lv.coefs[j,2:(num.lv+1)],lvs[i,])}
    for(j in 1:p) { y[i,j] ~ dpois(exp(lv.coefs[j,1] + eta[i,j])) }
    
  }
  ## Latent variables ##
  for(i in 1:n) { for(k in 1:num.lv) { lvs[i,k] ~ dnorm(0,1) } } 
  
  ## Process level and priors ##
  for(j in 1:p) { lv.coefs[j,1] ~ dnorm(0, tau = 0.1) } ## Separate response intercepts
  
  for(i in 1:(num.lv-1)) { for(j in (i+2):(num.lv+1)) { lv.coefs[i,j] <- 0 } } ## Constraints to 0 on upper diagonal
  for(i in 1:num.lv) { lv.coefs[i,i+1] ~ T(dnorm(0, tau = 0.1), 0,) } ## Sign constraints on diagonal elements
  for(i in 2:num.lv) { for(j in 2:i) { lv.coefs[i,j] ~ dnorm(0, tau = 0.1) } } ## Free lower diagonals
  for(i in (num.lv+1):p) { for(j in 2:(num.lv+1)) { lv.coefs[i,j] ~ dnorm(0, tau = 0.1) } } ## All other elements
})
dat <- list(y = spider$abund)
const <- list(
 p = ncol(dat$y),
 n = nrow(dat$y), 
 num.lv = 2)
# construct model object
model <- nimbleModel(pois.gllvm, const, dat, dimensions = list(lvs =  c(const$n, 2)))
mod <- nimble::compileNimble(model)
# test<-sampler_glm_pois(model,model,model$getParents("y",stochOnly = T,includeData = F),list())
# construct mcmc
nimbleMCMCconf <- configureMCMC(mod, monitors = c("lvs","lv.coefs"), print = FALSE)
nimbleMCMCconf$removeSamplers()
# block in the same as jags
library(R2jags)
mod2<-jags(data  = append(dat,const), model.file = "/home/bertv/boral/jagsboralmodel.txt", n.iter = 1000, n.burnin = 500, n.chains = 1, n.thin = 1, parameters.to.save = c("lvs","lv.coefs"))
samps <- ifelse(names(list.samplers(mod2$model))=="glm::Generic","sampler_glm_pois","AF_slice")
samps <- ifelse(lapply(list.samplers(mod2$model),length)==1,"slice",samps)

for(i in 1:length(samps)){
  nimbleMCMCconf$addSampler(rjags::list.samplers(mod2$model)[[i]], type = samps[i])  
}

nimbleMCMCb <- buildMCMC(nimbleMCMCconf)
nimbleMCMCc <- compileNimble(nimbleMCMCb, project = mod)
test_run <- runMCMC(nimbleMCMCc, niter = 1000, nburnin = 500, thin = 1, nchains = 1, samplesAsCodaMCMC = TRUE)
summary(test_run)# incorrect answer :(

```